{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3893a8c3-94fe-4903-a4d1-1114b54ee0f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk==3.6.7 in /opt/anaconda3/lib/python3.9/site-packages (3.6.7)\n",
      "Requirement already satisfied: click in /opt/anaconda3/lib/python3.9/site-packages (from nltk==3.6.7) (8.0.4)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/lib/python3.9/site-packages (from nltk==3.6.7) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/anaconda3/lib/python3.9/site-packages (from nltk==3.6.7) (2022.3.15)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.9/site-packages (from nltk==3.6.7) (4.64.0)\n",
      "Requirement already satisfied: gensim==4.1.2 in /opt/anaconda3/lib/python3.9/site-packages (4.1.2)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/anaconda3/lib/python3.9/site-packages (from gensim==4.1.2) (5.1.0)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /opt/anaconda3/lib/python3.9/site-packages (from gensim==4.1.2) (1.26.3)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /opt/anaconda3/lib/python3.9/site-packages (from gensim==4.1.2) (1.11.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk==3.6.7\n",
    "!pip install gensim==4.1.2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "import nltk as nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ffea7ac-1799-41e7-b009-54595a4dc993",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/danswk/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/danswk/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/danswk/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "183cc620-99d4-4752-8117-c31803946010",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this is an introduction data science course which introduces data science to beginners',\n",
       " 'machine learning for beginners']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "course1 = 'this is an introduction data science course which introduces data science to beginners'\n",
    "course2 = 'machine learning for beginners'\n",
    "courses = [course1, course2]\n",
    "courses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a37b7478-b826-4836-abdf-e9d267d4bb9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['this',\n",
       "  'is',\n",
       "  'an',\n",
       "  'introduction',\n",
       "  'data',\n",
       "  'science',\n",
       "  'course',\n",
       "  'which',\n",
       "  'introduces',\n",
       "  'data',\n",
       "  'science',\n",
       "  'to',\n",
       "  'beginners'],\n",
       " ['machine', 'learning', 'for', 'beginners']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_courses = [word_tokenize(course) for course in courses]  # tokenize sample courses\n",
    "tokenized_courses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91ec39d7-6e92-46c1-91a1-33f0b7be03f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'an': 0,\n",
       " 'beginners': 1,\n",
       " 'course': 2,\n",
       " 'data': 3,\n",
       " 'introduces': 4,\n",
       " 'introduction': 5,\n",
       " 'is': 6,\n",
       " 'science': 7,\n",
       " 'this': 8,\n",
       " 'to': 9,\n",
       " 'which': 10,\n",
       " 'for': 11,\n",
       " 'learning': 12,\n",
       " 'machine': 13}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_dict = gensim.corpora.Dictionary(tokenized_courses)  # create token dictionary\n",
    "tokens_dict.token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4ae20fc-dbbb-46b0-ab8d-e7f3e632c312",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1),\n",
       "  (1, 1),\n",
       "  (2, 1),\n",
       "  (3, 2),\n",
       "  (4, 1),\n",
       "  (5, 1),\n",
       "  (6, 1),\n",
       "  (7, 2),\n",
       "  (8, 1),\n",
       "  (9, 1),\n",
       "  (10, 1)],\n",
       " [(1, 1), (11, 1), (12, 1), (13, 1)]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "courses_bow = [tokens_dict.doc2bow(course) for course in tokenized_courses]  # generate BoW features for each tokenized course\n",
    "courses_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4778d892-a02c-442f-afad-9e7fc0745ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of words for course 0\n",
      "-------------------------\n",
      "- token: an, count: 1\n",
      "- token: beginners, count: 1\n",
      "- token: course, count: 1\n",
      "- token: data, count: 2\n",
      "- token: introduces, count: 1\n",
      "- token: introduction, count: 1\n",
      "- token: is, count: 1\n",
      "- token: science, count: 2\n",
      "- token: this, count: 1\n",
      "- token: to, count: 1\n",
      "- token: which, count: 1\n",
      "=========================\n",
      "Bag of words for course 1\n",
      "-------------------------\n",
      "- token: beginners, count: 1\n",
      "- token: for, count: 1\n",
      "- token: learning, count: 1\n",
      "- token: machine, count: 1\n",
      "=========================\n"
     ]
    }
   ],
   "source": [
    "for course_idx, course_bow in enumerate(courses_bow):\n",
    "    print(f'Bag of words for course {course_idx}\\n-------------------------')\n",
    "    \n",
    "    # print bow value (count) for each token index (word)\n",
    "    for token_index, token_bow in course_bow:\n",
    "        token = tokens_dict.get(token_index)\n",
    "        print(f'- token: {token}, count: {token_bow}')\n",
    "\n",
    "    print('=========================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7988e97-7c66-4477-b40d-c2cea740dc3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))  # get English stop words\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80adca8b-2535-42ad-8095-77399b05df04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'is',\n",
       " 'an',\n",
       " 'introduction',\n",
       " 'data',\n",
       " 'science',\n",
       " 'course',\n",
       " 'which',\n",
       " 'introduces',\n",
       " 'data',\n",
       " 'science',\n",
       " 'to',\n",
       " 'beginners']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_courses[0]  # get course 1 tokens with stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "455571e5-27f3-4fe1-889a-30d3ef2b948f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['introduction',\n",
       " 'data',\n",
       " 'science',\n",
       " 'course',\n",
       " 'introduces',\n",
       " 'data',\n",
       " 'science',\n",
       " 'beginners']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_tokens = [w for w in tokenized_courses[0] if not w.lower() in stop_words]  # get course 1 tokens without stop words\n",
    "processed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "771c633b-3e18-4460-a218-d332b4762722",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('this', 'DT'),\n",
       " ('is', 'VBZ'),\n",
       " ('an', 'DT'),\n",
       " ('introduction', 'NN'),\n",
       " ('data', 'NNS'),\n",
       " ('science', 'NN'),\n",
       " ('course', 'NN'),\n",
       " ('which', 'WDT'),\n",
       " ('introduces', 'VBZ'),\n",
       " ('data', 'NNS'),\n",
       " ('science', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('beginners', 'NNS')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags = nltk.pos_tag(tokenized_courses[0])  # analyze part of speech (POS), annotate each word\n",
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cfeffc8f-4ef6-4e94-a171-070169f6124f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "COURSE_ID                                               ML0201EN\n",
       "TITLE          robots are coming  build iot apps with watson ...\n",
       "DESCRIPTION    have fun with iot and learn along the way  if ...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# COURSE CONTENT DATASET\n",
    "course_url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML321EN-SkillsNetwork/labs/datasets/course_processed.csv\"\n",
    "course_content_df = pd.read_csv(course_url)\n",
    "course_content_df.iloc[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46da7787-7449-4694-9600-1a94bba240e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "course_content_df['course_texts'] = course_content_df[['TITLE', 'DESCRIPTION']].agg(' '.join, axis=1)  # merge course titles, descriptions\n",
    "course_content_df = course_content_df.reset_index()\n",
    "course_content_df['index'] = course_content_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4aa4a4b6-c81b-4cc6-b684-e450c728dbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_course(course, keep_only_nouns=True):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(course)\n",
    "    word_tokens = [w for w in word_tokens if (not w.lower() in stop_words) and (not w.isnumeric())]  # remove English stop words, numbers\n",
    "    \n",
    "    # keep only nouns\n",
    "    if keep_only_nouns:\n",
    "        filter_list = ['WDT', 'WP', 'WRB', 'FW', 'IN', 'JJR', 'JJS', 'MD',\n",
    "                       'PDT', 'POS', 'PRP', 'RB', 'RBR', 'RBS', 'RP']\n",
    "        tags = nltk.pos_tag(word_tokens)\n",
    "        word_tokens = [word for word, pos in tags if pos not in filter_list]\n",
    "\n",
    "    return word_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0413833-2927-4c73-85b7-bbe3d62d88e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'robots are coming  build iot apps with watson  swift  and node red have fun with iot and learn along the way  if you re a swift developer and want to learn more about iot and watson ai services in the cloud  raspberry pi   and node red  you ve found the right place  you ll build iot apps to read temperature data  take pictures with a raspcam  use ai to recognize the objects in those pictures  and program an irobot create 2 robot  '"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "course_1 = course_content_df.iloc[0, :]['course_texts']\n",
    "course_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1bcc8117-f5d5-4b2a-9158-26e0cc90a367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['robots',\n",
       " 'coming',\n",
       " 'build',\n",
       " 'iot',\n",
       " 'apps',\n",
       " 'watson',\n",
       " 'swift',\n",
       " 'red',\n",
       " 'fun',\n",
       " 'iot',\n",
       " 'learn',\n",
       " 'way',\n",
       " 'swift',\n",
       " 'developer',\n",
       " 'want',\n",
       " 'learn',\n",
       " 'iot',\n",
       " 'watson',\n",
       " 'ai',\n",
       " 'services',\n",
       " 'cloud',\n",
       " 'raspberry',\n",
       " 'pi',\n",
       " 'node',\n",
       " 'red',\n",
       " 'found',\n",
       " 'place',\n",
       " 'build',\n",
       " 'iot',\n",
       " 'apps',\n",
       " 'read',\n",
       " 'temperature',\n",
       " 'data',\n",
       " 'take',\n",
       " 'pictures',\n",
       " 'raspcam',\n",
       " 'use',\n",
       " 'ai',\n",
       " 'recognize',\n",
       " 'objects',\n",
       " 'pictures',\n",
       " 'program',\n",
       " 'irobot',\n",
       " 'create',\n",
       " 'robot']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_course(course_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6cfe8023-f05f-4723-aeae-fd690761b37f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['robots',\n",
       "  'coming',\n",
       "  'build',\n",
       "  'iot',\n",
       "  'apps',\n",
       "  'watson',\n",
       "  'swift',\n",
       "  'red',\n",
       "  'fun',\n",
       "  'iot',\n",
       "  'learn',\n",
       "  'way',\n",
       "  'swift',\n",
       "  'developer',\n",
       "  'want',\n",
       "  'learn',\n",
       "  'iot',\n",
       "  'watson',\n",
       "  'ai',\n",
       "  'services',\n",
       "  'cloud',\n",
       "  'raspberry',\n",
       "  'pi',\n",
       "  'node',\n",
       "  'red',\n",
       "  'found',\n",
       "  'place',\n",
       "  'build',\n",
       "  'iot',\n",
       "  'apps',\n",
       "  'read',\n",
       "  'temperature',\n",
       "  'data',\n",
       "  'take',\n",
       "  'pictures',\n",
       "  'raspcam',\n",
       "  'use',\n",
       "  'ai',\n",
       "  'recognize',\n",
       "  'objects',\n",
       "  'pictures',\n",
       "  'program',\n",
       "  'irobot',\n",
       "  'create',\n",
       "  'robot'],\n",
       " ['accelerating',\n",
       "  'deep',\n",
       "  'learning',\n",
       "  'gpu',\n",
       "  'training',\n",
       "  'complex',\n",
       "  'deep',\n",
       "  'learning',\n",
       "  'models',\n",
       "  'large',\n",
       "  'datasets',\n",
       "  'takes',\n",
       "  'time',\n",
       "  'course',\n",
       "  'learn',\n",
       "  'use',\n",
       "  'accelerated',\n",
       "  'gpu',\n",
       "  'hardware',\n",
       "  'overcome',\n",
       "  'scalability',\n",
       "  'problem',\n",
       "  'learning',\n",
       "  'use',\n",
       "  'accelerated',\n",
       "  'hardware',\n",
       "  'google',\n",
       "  'tensor',\n",
       "  'processing',\n",
       "  'unit',\n",
       "  'tpu',\n",
       "  'nvidia',\n",
       "  'gpu',\n",
       "  'accelerate',\n",
       "  'convolutional',\n",
       "  'neural',\n",
       "  'network',\n",
       "  'computations',\n",
       "  'time',\n",
       "  'cloud',\n",
       "  'chips',\n",
       "  'designed',\n",
       "  'support',\n",
       "  'training',\n",
       "  'neural',\n",
       "  'networks',\n",
       "  'trained',\n",
       "  'networks',\n",
       "  'inference',\n",
       "  'accelerated',\n",
       "  'hardware',\n",
       "  'reduce',\n",
       "  'training',\n",
       "  'time',\n",
       "  'problem',\n",
       "  'data',\n",
       "  'sensitiveand',\n",
       "  'feel',\n",
       "  'comfortable',\n",
       "  'uploading',\n",
       "  'public',\n",
       "  'cloud',\n",
       "  'preferring',\n",
       "  'analyze',\n",
       "  'premise',\n",
       "  'case',\n",
       "  'need',\n",
       "  'use',\n",
       "  'house',\n",
       "  'system',\n",
       "  'gpu',\n",
       "  'support',\n",
       "  'one',\n",
       "  'solution',\n",
       "  'use',\n",
       "  'ibm',\n",
       "  'power',\n",
       "  'systems',\n",
       "  'gpu',\n",
       "  'power',\n",
       "  'ai',\n",
       "  'power',\n",
       "  'ai',\n",
       "  'platform',\n",
       "  'supports',\n",
       "  'popular',\n",
       "  'machine',\n",
       "  'learning',\n",
       "  'libraries',\n",
       "  'dependencies',\n",
       "  'including',\n",
       "  'tensorflow',\n",
       "  'caffe',\n",
       "  'torch',\n",
       "  'theano',\n",
       "  'course',\n",
       "  'understand',\n",
       "  'gpu',\n",
       "  'based',\n",
       "  'accelerated',\n",
       "  'hardware',\n",
       "  'benefit',\n",
       "  'deep',\n",
       "  'learning',\n",
       "  'scaling',\n",
       "  'needs',\n",
       "  'deploy',\n",
       "  'deep',\n",
       "  'learning',\n",
       "  'networks',\n",
       "  'gpu',\n",
       "  'accelerated',\n",
       "  'hardware',\n",
       "  'several',\n",
       "  'problems',\n",
       "  'including',\n",
       "  'classification',\n",
       "  'images',\n",
       "  'videos']]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize courses in courses_df\n",
    "tokenized_courses = []\n",
    "for content in course_content_df.course_texts:\n",
    "    tokenized_course = tokenize_course(content)\n",
    "    tokenized_courses.append(tokenized_course)\n",
    "    \n",
    "tokenized_courses[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cbb05a3d-5f8c-46d1-953c-7319e8e8d1f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ai: 0\n",
      "apps: 1\n",
      "build: 2\n",
      "cloud: 3\n",
      "coming: 4\n",
      "create: 5\n",
      "data: 6\n",
      "developer: 7\n",
      "found: 8\n",
      "fun: 9\n"
     ]
    }
   ],
   "source": [
    "tokens_dict = gensim.corpora.Dictionary(tokenized_courses)\n",
    "\n",
    "# preview token dictionary\n",
    "for key, value in list(tokens_dict.token2id.items())[:10]:\n",
    "    print(f'{key}: {value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c342355-4c93-41d1-bd77-304141f8bed4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 2),\n",
       " (1, 2),\n",
       " (2, 2),\n",
       " (3, 1),\n",
       " (4, 1),\n",
       " (5, 1),\n",
       " (6, 1),\n",
       " (7, 1),\n",
       " (8, 1),\n",
       " (9, 1),\n",
       " (10, 4),\n",
       " (11, 1),\n",
       " (12, 2),\n",
       " (13, 1),\n",
       " (14, 1),\n",
       " (15, 1),\n",
       " (16, 2),\n",
       " (17, 1),\n",
       " (18, 1),\n",
       " (19, 1),\n",
       " (20, 1),\n",
       " (21, 1),\n",
       " (22, 1),\n",
       " (23, 2),\n",
       " (24, 1),\n",
       " (25, 1),\n",
       " (26, 1),\n",
       " (27, 2),\n",
       " (28, 1),\n",
       " (29, 1),\n",
       " (30, 1),\n",
       " (31, 1),\n",
       " (32, 2),\n",
       " (33, 1)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "courses_bow = [tokens_dict.doc2bow(course) for course in tokenized_courses]\n",
    "courses_bow[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5e91f388-194c-4795-8dbe-401541160f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_docs = [courses_bow[i] for i in range(len(tokenized_courses))]\n",
    "doc_indices = []\n",
    "doc_ids = []\n",
    "tokens = []\n",
    "bows = []\n",
    "\n",
    "for doc_index, doc_bow in enumerate(bow_docs):\n",
    "    for token_index, token_bow in doc_bow:\n",
    "        doc_indices.append(course_content_df.iloc[doc_index, 0])\n",
    "        doc_ids.append(course_content_df.iloc[doc_index, 1])\n",
    "        tokens.append(tokens_dict.get(token_index))\n",
    "        bows.append(token_bow)\n",
    "\n",
    "    bow_dicts = {'doc_index':doc_indices,\n",
    "                 'doc_id':doc_ids,\n",
    "                 'token':tokens,\n",
    "                 'bow':bows}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e37f659a-c5c0-4fee-ad38-22a38dbf0460",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_index</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>token</th>\n",
       "      <th>bow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ML0201EN</td>\n",
       "      <td>ai</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>ML0201EN</td>\n",
       "      <td>apps</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>ML0201EN</td>\n",
       "      <td>build</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>ML0201EN</td>\n",
       "      <td>cloud</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>ML0201EN</td>\n",
       "      <td>coming</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>ML0201EN</td>\n",
       "      <td>create</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>ML0201EN</td>\n",
       "      <td>data</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>ML0201EN</td>\n",
       "      <td>developer</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>ML0201EN</td>\n",
       "      <td>found</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>ML0201EN</td>\n",
       "      <td>fun</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>ML0201EN</td>\n",
       "      <td>iot</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>ML0201EN</td>\n",
       "      <td>irobot</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>ML0201EN</td>\n",
       "      <td>learn</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>ML0201EN</td>\n",
       "      <td>node</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>ML0201EN</td>\n",
       "      <td>objects</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>ML0201EN</td>\n",
       "      <td>pi</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>ML0201EN</td>\n",
       "      <td>pictures</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>ML0201EN</td>\n",
       "      <td>place</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>ML0201EN</td>\n",
       "      <td>program</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>ML0201EN</td>\n",
       "      <td>raspberry</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>ML0201EN</td>\n",
       "      <td>raspcam</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>ML0201EN</td>\n",
       "      <td>read</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>ML0201EN</td>\n",
       "      <td>recognize</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>ML0201EN</td>\n",
       "      <td>red</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>ML0201EN</td>\n",
       "      <td>robot</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>ML0201EN</td>\n",
       "      <td>robots</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>ML0201EN</td>\n",
       "      <td>services</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>ML0201EN</td>\n",
       "      <td>swift</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>ML0201EN</td>\n",
       "      <td>take</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>ML0201EN</td>\n",
       "      <td>temperature</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0</td>\n",
       "      <td>ML0201EN</td>\n",
       "      <td>use</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0</td>\n",
       "      <td>ML0201EN</td>\n",
       "      <td>want</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0</td>\n",
       "      <td>ML0201EN</td>\n",
       "      <td>watson</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0</td>\n",
       "      <td>ML0201EN</td>\n",
       "      <td>way</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1</td>\n",
       "      <td>ML0122EN</td>\n",
       "      <td>ai</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1</td>\n",
       "      <td>ML0122EN</td>\n",
       "      <td>cloud</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1</td>\n",
       "      <td>ML0122EN</td>\n",
       "      <td>data</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1</td>\n",
       "      <td>ML0122EN</td>\n",
       "      <td>learn</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1</td>\n",
       "      <td>ML0122EN</td>\n",
       "      <td>use</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1</td>\n",
       "      <td>ML0122EN</td>\n",
       "      <td>accelerate</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    doc_index    doc_id        token  bow\n",
       "0           0  ML0201EN           ai    2\n",
       "1           0  ML0201EN         apps    2\n",
       "2           0  ML0201EN        build    2\n",
       "3           0  ML0201EN        cloud    1\n",
       "4           0  ML0201EN       coming    1\n",
       "5           0  ML0201EN       create    1\n",
       "6           0  ML0201EN         data    1\n",
       "7           0  ML0201EN    developer    1\n",
       "8           0  ML0201EN        found    1\n",
       "9           0  ML0201EN          fun    1\n",
       "10          0  ML0201EN          iot    4\n",
       "11          0  ML0201EN       irobot    1\n",
       "12          0  ML0201EN        learn    2\n",
       "13          0  ML0201EN         node    1\n",
       "14          0  ML0201EN      objects    1\n",
       "15          0  ML0201EN           pi    1\n",
       "16          0  ML0201EN     pictures    2\n",
       "17          0  ML0201EN        place    1\n",
       "18          0  ML0201EN      program    1\n",
       "19          0  ML0201EN    raspberry    1\n",
       "20          0  ML0201EN      raspcam    1\n",
       "21          0  ML0201EN         read    1\n",
       "22          0  ML0201EN    recognize    1\n",
       "23          0  ML0201EN          red    2\n",
       "24          0  ML0201EN        robot    1\n",
       "25          0  ML0201EN       robots    1\n",
       "26          0  ML0201EN     services    1\n",
       "27          0  ML0201EN        swift    2\n",
       "28          0  ML0201EN         take    1\n",
       "29          0  ML0201EN  temperature    1\n",
       "30          0  ML0201EN          use    1\n",
       "31          0  ML0201EN         want    1\n",
       "32          0  ML0201EN       watson    2\n",
       "33          0  ML0201EN          way    1\n",
       "34          1  ML0122EN           ai    2\n",
       "35          1  ML0122EN        cloud    2\n",
       "36          1  ML0122EN         data    1\n",
       "37          1  ML0122EN        learn    1\n",
       "38          1  ML0122EN          use    4\n",
       "39          1  ML0122EN   accelerate    1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "course_bow_df = pd.DataFrame(bow_dicts)  # create dataframe of extracted BoW features\n",
    "course_bow_df.head(40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
